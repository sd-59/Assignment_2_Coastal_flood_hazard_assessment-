{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# **Exercise 4:** Extreme value analysis\n",
    "\n",
    "## Estimating extreme sea levels using the Gumbel distribution to provide coastal boundary conditions\n",
    "\n",
    "In the lecture, you were taught about ways to carry out frequency analyses, and to estimate sea levels for different return periods. You will apply the Gumbel distribution to an hourly time-series of sea levels for Charleston, SC. This includes the following steps:\n",
    "1. **Download sea-level data**\n",
    "2. **Pre-processing of sea-level data**  \n",
    "    a. Read sea-level data  \n",
    "    b. Detrend the sea level data  \n",
    "5. **Extreme valye analysis**  \n",
    "   a. Extract annual maxima  \n",
    "    b. Fit Gumbel distribution  \n",
    "   c. Calculate confidence intervals  \n",
    "8. **Create design hydrographs**   \n",
    "\n",
    "Each step is explained in more detail below.\n",
    "\n",
    "Note that where it states `<your value>`, you will have to change the code to make it run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 0. Import packages\n",
    "\n",
    "We first need to import different python packages. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np              #used to work with 1d/2d/3d arrays, endless possibilities\n",
    "import pandas as pd             #used to give your array a column header and row index, simplifies indexing\n",
    "from pathlib import Path\n",
    "import scipy.stats as sp        #used to perform statistical analysis\n",
    "import matplotlib.pyplot as plt #used for making plots\n",
    "from pandas.plotting import register_matplotlib_converters #used for converting between e.g. pandas timestamp/numpy datetime64/datetime.datetime\n",
    "\n",
    "# local script used to download hourly sea levels from the NOAA database\n",
    "from retrieve_water_levels import download_data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Download sea-level data\n",
    "\n",
    "Here you dowload the hourly sea-level data of a tide gauge station of your choice. You do this by setting the value of `station_id`. You can find this on the website of the National Oceanic and Atmospheric Administration (NOAA) Tides and Currents database (https://tidesandcurrents.noaa.gov/map/). You can also change the desired period by setting the value of `start_year` and `end_year`. \n",
    "\n",
    "The dowloaded data is saved in a csv file in the current directory. Watch the print messages to make sure that data is avaible for the years of your choice. \n",
    "\n",
    "For this exercise, we use the tide gauge Charleston, CS . Download the hourly sea-level data for the period 1980-present in .csv format by filling in the block of code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# User inputs: uncomment and fill in your values\n",
    "# station_name = '<your value>'\n",
    "# station_id = '<your value>'\n",
    "# start_year = '<your value>'\n",
    "# end_year = '<your value>'\n",
    "\n",
    "# complete folder path. Prefix r to produce a raw string. Without this r your folder name might be interpreted as Unicode resulting in a syntax error.\n",
    "# We suggest to store the data in a folder named 'data' \n",
    "obs_folder = Path(r'../data/water_levels') \n",
    "filename = obs_folder.joinpath(f'hourly_water_levels_{station_id}.csv') #file name \n",
    "\n",
    "# comment out the next line if you already have the data downloaded\n",
    "_ = download_data(filename, station_id=station_id, start_year=start_year, end_year=end_year, datum='MSL')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2a. Read sea-level data\n",
    "The second step is to open the csv file as a so-called pandas dataframe. We open the excel file with pd.read_csv which stands for comma separated values. \n",
    "\n",
    "Run the first cell below. Once the asterisk sign changes into a number, the process is finished. If an error appears, check whether you correctly defined the folder path. \n",
    "\n",
    "At this point the dataset ranges from 1980-2022. For our analysis we want to focus on the period 1991-2020. We can easily do this by locating the rows for which the column 'datetime' index has a year value between 1990 and 2021. To do so, run the next cell and make sure to compare the printed output with the previous cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the csv file with hourly sealevel time series\n",
    "waterlevel = pd.read_csv(\n",
    "    filename, \n",
    "    index_col=0,\n",
    "    parse_dates=True,\n",
    ") \n",
    "waterlevel.iloc[[0,1,2,-3,-2,-1],:]  # Print the first three and last three rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# User inputs: uncomment and fill in your values\n",
    "# Locate rows for which column value 'year' is within 1991-2020\n",
    "# yrmin = '<your value>'\n",
    "# yrmax = '<your value>'\n",
    "waterlevel = waterlevel.loc[(waterlevel.index.year > yrmin) & (waterlevel.index.year < yrmax)] \n",
    "waterlevel[waterlevel == -32767] = np.nan                       # Change no data values to nan\n",
    "waterlevel.iloc[[0,1,2,-3,-2,-1],:]                             # Print the first three and last three rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2b. Detrend sea-level data\n",
    "\n",
    "The sea-levels are 'absolute sea-levels'. This means that the data includes sea-level rise, vertical land motion and other processes. For extreme value analysis the data needs to be stationary. Therefore, we are going to detrend the hourly sea levels. We also convert the units from mm to m. \n",
    "\n",
    "We will plot the data to observe the trend over the period 1991-2020. The last available date is used as reference to detrend the sea-level data. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>**Q1 How much is the sea-level trend over the period 1990-2020? And is this trend sensitive to any of the settings?**<u>\n",
    "\n",
    "Answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def lin_detrend_wNa(data, ref_date, remove_means = True, figure_plotting = True):\n",
    "    \"\"\"arguments:\n",
    "        data is a pd.Series with date as index \n",
    "        ref_date: if a date is mentioned, remove trend taking the sealevel on this date as ref\n",
    "        remove_means: if True, centers the detrended timeseries around 0\n",
    "        figure_plotting: if True returns a figure of both timeseries\n",
    "    returns:\n",
    "        the linearly detrended data with time as index\"\"\"\n",
    "     \n",
    "    y = np.array(data.waterlevel) # sealevel time series\n",
    "    x = np.arange(0,len(y),1)     # create x-values ranging from 0 to 1 corresponding to the sealevel timeseries (y)\n",
    "    not_nan_ind = ~np.isnan(y)    # create boolean, all indexes that are not nan become 'True'\n",
    "    #regression line: m=slope; b=intercept; r_val=correlation coefficient; p_val=two-sided p-value; std_err=standard error of the estimated gradient\n",
    "    m, b, r_val, p_val, std_err = sp.linregress(x[not_nan_ind],y[not_nan_ind]) # see docs 'https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.linregress.html'\n",
    "   \n",
    "    if remove_means == True:                          # if true, values will be centered around the mean, such that the mean of the complete time series becomes zero\n",
    "        detrend_y = y - (m*x + b)                     # y = original time series; m = slope regression line; x = x-values range; b = intercept of the regression line;\n",
    "    elif ref_date is not None:                        # elif ref_date is given, detrend with this date as reference point\n",
    "        x_0 = np.flatnonzero(data.index == ref_date)  # index of the refdate used to detrend the complete time series\n",
    "        detrend_y = y - (m*x + b) + (m * x_0 + b)     # y = original time series; m = slope regression line; x = x-values range; b = intercept of the regression line;\n",
    "    else:                                             # else ref_date automatically becomes the first timestep of the time series\n",
    "        detrend_y = y - (m*x)                         # y = original time series; m = slope regression line; x = x-values range;\n",
    "    \n",
    "    print('Linear trend is: ', m)\n",
    "    \n",
    "    if figure_plotting == True:\n",
    "        register_matplotlib_converters()\n",
    "        plt.figure()\n",
    "        plt.plot(data.index, y, label = 'original')\n",
    "        plt.plot(data.index, detrend_y, label = 'detrended')\n",
    "        plt.xlabel('year')\n",
    "        plt.ylabel('waterlevel [m]')\n",
    "        plt.legend(loc='upper right')\n",
    "    \n",
    "    result = pd.DataFrame(data = detrend_y, index = data.index, columns=['sealevel_det']) # create a pandas dataframe with the detrended time series and dates as index\n",
    "    return m,result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Here we call the function defined in the cell above.\n",
    "m,result=lin_detrend_wNa(\n",
    "    waterlevel, \n",
    "    waterlevel.index[-1], \n",
    "    remove_means=False,\n",
    "    figure_plotting=True\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "waterlevel = pd.concat([waterlevel,result],axis=1) # Concatenates the Charleston, SC dataframe and the new detrended waterlevel\n",
    "waterlevel.iloc[[0,1,2,-3,-2,-1],:]                 # Print the first three and last three rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3a. Extract annual maxima\n",
    "\n",
    "The annual maximum sea-levels will be stored in the variable `sl_annmax`. To get an idea of the magnitude of the annual maxima, you are going to plot them. For this you will have to write a short script yourself. Make sure you plot the `rel_sl_annmax`. While the sl_annmax values are the absolute yearly maxima, referenced to mean sea level, it is more insightful to use the mean of our timeseries as reference. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>**Q2: What is the range of annual maxima? Do you see a trend in the annual maximum sea-level time-series?**<u>\n",
    "\n",
    "Q2 answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sl_annmax = waterlevel.groupby(waterlevel.index.year)['sealevel_det'].max()                                        # extract annual maxima from the detrended sealevel time series\n",
    "rel_sl_annmax = waterlevel.groupby(waterlevel.index.year)['sealevel_det'].max()-waterlevel['sealevel_det'].mean()    # subtract the mean sea level (MSL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Use this cell to make a plot of the annual maxima\n",
    "#Check out matplotlib and its possibilities. This python library has been imported at the start of this script as 'plt'\n",
    "# Make sure to include appropriate labels and also plot the absolute sea levels to check the effect of substracting the mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3b. Fit a gumbel distribution to the annual maxima\n",
    "\n",
    "Now we are ready to fit a gumbel distribution to our data. First, we calculate the location and scale parameter of our distribution by making use of the gumbel_r function that is included in the scipy.statspackage that was imported at the start of this script. Our data is right-skewed because less extreme sea levels (left side of the distribution) are more common than more extreme sea levels (right side of the distribution). Therefore we make use of the gumbel_r function. \n",
    "\n",
    "The next step is to rank our values from highest (rank 1) to lowest (rank 30) and compute the empirical exceedance probability. The empirical exceedance probability indicates the chance that a certain water level rank will be exceeded in any given year (x100%). By dividing 1 through the cumulative probability, we can determine the return period that belongs to each rank. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Fit Gumbel distribution\n",
    "[location,scale] = sp.gumbel_r.fit(rel_sl_annmax)  # fit the gumbel_r distribution to our data to obtain estimates of the location and scale parameter\n",
    "\n",
    "#Step 1\n",
    "rank = rel_sl_annmax.rank(axis=0, ascending=False) # rank the annual maxima\n",
    "emp_exc_prob = rank/(rank.size+1)                  # empirical exceedance probabilities of the annual maxima\n",
    "emp_cum_prob = 1 - emp_exc_prob                    # empirical cumalative probabilities of the annual maxima (= inverse of exceedance probability)\n",
    "emp_rp = 1/emp_exc_prob                            # return periods of the annual maxima\n",
    "\n",
    "#Step 2\n",
    "cum_prob_x = np.arange(0.01,1,0.001)               # create 2d numpy array with probability range 0.01-1, with steps of 0.001\n",
    "exc_prob_x = 1 - cum_prob_x                        # exceedance probabilities\n",
    "rp_x=1/(exc_prob_x)                                # return periods\n",
    "\n",
    "#Step 3\n",
    "gumbel_variate = -np.log(-np.log(cum_prob_x))      # take the double log, also see 'https://en.wikipedia.org/wiki/Gumbel_distribution#Computational_methods'\n",
    "\n",
    "#Step 4\n",
    "return_periods = np.array([2,5,10,25,50,100,250,500,1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot extreme value distribution\n",
    "\n",
    "Below we create multiple figures that visualizes the different steps to fit a Gumbel distribution to the annual maxima. Above each figure code a short explanation about what the figure shows is provided."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>**Q3: What is the approximate value of relative sea-level for a 10-year return period? What about a 100-year return period? Has such an extreme event occured in the period 1990-2020?**<u>\n",
    "\n",
    "Q3 answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Figure 1: Gumbel distribution and data plotted with normal x and y axes\n",
    "plt.figure()\n",
    "plt.plot(emp_rp,rel_sl_annmax,'bo')\n",
    "plt.plot(rp_x,sp.gumbel_r.ppf(cum_prob_x,location,scale))\n",
    "plt.xlabel('Return Period (yrs)')\n",
    "plt.ylabel('Value [m]')\n",
    "\n",
    "#Figure 2: Same as Figure 1, but now the x-axis scale is logarithmic\n",
    "plt.figure()\n",
    "plt.semilogx(emp_rp,rel_sl_annmax,'bo')\n",
    "plt.semilogx(rp_x,sp.gumbel_r.ppf(cum_prob_x,location,scale))\n",
    "plt.xlabel('Return Period (yrs)')\n",
    "plt.ylabel('Value [m]')\n",
    "\n",
    "#Figure 3: Gumbel variate on the x-axis -> double log scale used for plotting\n",
    "plt.figure()\n",
    "plt.plot(-np.log(-np.log(emp_cum_prob)),rel_sl_annmax, 'ob')\n",
    "plt.plot(gumbel_variate,sp.gumbel_r.ppf(cum_prob_x,location,scale))\n",
    "plt.xlabel('Gumbel variate = -ln(-ln($P_{cum}$))')\n",
    "plt.ylabel('Value [m]')\n",
    "\n",
    "#Figure 4: Same as Figure 3, but now the x-axis shows the return period instead of the Gumbel variate\n",
    "plt.figure()\n",
    "plt.plot(-np.log(-np.log(emp_cum_prob)),rel_sl_annmax, 'ob')\n",
    "plt.plot(gumbel_variate,sp.gumbel_r.ppf(cum_prob_x,location,scale))\n",
    "labels = [str(i) for i in return_periods]\n",
    "plt.xticks(ticks = -np.log(-np.log(1 - 1/return_periods)), labels = labels)\n",
    "plt.grid()\n",
    "plt.xlabel('Return Period (yrs)')\n",
    "plt.ylabel('Value [m]');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the code in the following cell, you can estimate the return period for sea-levels with any different return period precisely, instead of reading them from the graph above. You may adjust the requested 'return_periods' yourself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>**Q4: What is the value of relative sea-level for a 100-year return period? And for a 55-year return period?**<u>\n",
    "\n",
    "Q4 answer: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "return_periods = np.array([2,5,10,25,50,100,250,500,1000])\n",
    "probs = 1./return_periods                                               # exceedance probabilities\n",
    "rps_out = [return_periods,sp.gumbel_r.ppf(1-probs,location,scale)]      # Percent Point Function (PPF); inverse of CDF-percentiles (CDF = cumulative distribution function)\n",
    "df = pd.DataFrame({'return periods (yrs)': rps_out[0],'relative sealevel (m)': rps_out[1]}) # make a dataframe of it: easier to look at the data\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importantly, in order to express the sea-level in absolute terms, we need to add the relative sea-levels to the long-term mean sea-level. This is carried out in the next cell.\n",
    "\n",
    "<u>**Q4: What is the 100 year return period of absolute sea-level?**<u>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q4 answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df['absolute sealevel (m)'] = df['relative sealevel (m)']+waterlevel['sealevel_det'].mean() # sum the relative sealevel and the detrended sealevel timeseries\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other extreme value distributions\n",
    "Gumbel is an example of a linear distribution which has a scale and location parameter to describe them. Most other distributions have a third parameter. Look up what other types of distributions exist. Try adjusting the lines of code below that we copied from the gumbel fitting procedure. Instead of 'gumbel_r' try fitting the 'genextreme' distribution. See the following link for more information on how to do this: https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.genextreme.html#scipy.stats.genextreme\n",
    "    \n",
    "<u>**Q5: What is the name of this third parameter? Would you prefer to fit a distribution with three parameters to your sea-level data? Explain your answer.**<u>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q5 answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "[location,scale] = sp.gumbel_r.fit(rel_sl_annmax)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(-np.log(-np.log(emp_cum_prob)),rel_sl_annmax, 'ob')\n",
    "plt.plot(gumbel_variate,sp.gumbel_r.ppf(cum_prob_x,location,scale))\n",
    "labels = [str(i) for i in return_periods]\n",
    "plt.xticks(ticks = -np.log(-np.log(1 - 1/return_periods)), labels = labels)\n",
    "plt.grid()\n",
    "plt.xlabel('Return Period (yrs)')\n",
    "plt.ylabel('Value [m]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3c. Calculate confidence intervals\n",
    "\n",
    "The final step is about understanding the uncertainty associated with the estimated distribution. You will compute the 90% statistical confidence intervals of the gumbel distribution (i.e. 5th-95th percentile) using a bootstrapping approach. Bootstrapping involves the resampling a single dataset to create many simulated samples, as a way to estimate the uncertainty. This means you randomly select 30 values from the annual maxima and put each value back in the sample afterwards. This procedure is repeated 599 times following Wilcox et al. (2010). Subsequently, 599 location and scale parameters are calculated from which the 5th and 95th percentile of the parameter estimates can be extracted. Now we plot figure 4 again, including the 90% confidence interval.\n",
    "\n",
    "When working on your own research area in a few days, we expect you to understand the uncertainty of your data. So make sure you understand the bootstrapping approach!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# prepare bootstrapping\n",
    "nr = 599 # number of bootstrap repetitions Wilcox, R. R. (2010). Fundamentals of modern statistical methods: Substantially improving power and accuracy. Springer.\n",
    "ns = rel_sl_annmax.shape[0]                       # number of annual maxima\n",
    "water_levels = np.zeros((nr, cum_prob_x.size))    # array with estimated water levels\n",
    "loc,sca = sp.gumbel_r.fit(rel_sl_annmax) # location and scale parameter when gumbel is fitted to annual maxima (1980-2009)\n",
    "location = np.zeros((nr,))               # array with zeros\n",
    "scale = np.zeros_like(location)          # array with zeros\n",
    "peaks = rel_sl_annmax.values             # annual maxima\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# perform bootstrapping\n",
    "for rpi in range(nr):                                     # bootstrap for-loop (599 repetitions)\n",
    "    boot = np.random.choice(peaks, size=ns, replace=True) # randomly select 30 samples from the 30 annual maxima, with replacement\n",
    "    location[rpi],scale[rpi] = sp.gumbel_r.fit(boot)      # fit the gumbel distribution to obtain location and scale estimates\n",
    "    water_levels[rpi, :] = sp.gumbel_r.ppf(cum_prob_x,location[rpi],scale[rpi]) # estimate water levels with fitted parrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# extract 5th and 95th percentile water level estimates\n",
    "water_levels5 =  np.percentile(water_levels, 5, axis = 0)\n",
    "water_levels95 =  np.percentile(water_levels, 95, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# plot Figure 4 again, now with the 90% confidence interval\n",
    "plt.figure()\n",
    "plt.plot(-np.log(-np.log(emp_cum_prob)),rel_sl_annmax, 'ob',label='empirical distribution (Weibull)')\n",
    "plt.plot(gumbel_variate,sp.gumbel_r.ppf(cum_prob_x,loc,sca),label='gumbel distribution')\n",
    "# fill_between takes three arguments: x; y1 (5th percentile, lower bound); y2 (95th percentile, upper bound)\n",
    "# alpha makes the color transparent\n",
    "plt.fill_between(gumbel_variate,water_levels5 ,water_levels95, alpha=0.4,label='90% confidence interval')\n",
    "labels = [str(i) for i in return_periods]\n",
    "plt.legend(loc='upper left')\n",
    "plt.xticks(ticks = -np.log(-np.log(1 - 1/return_periods)), labels = labels)\n",
    "plt.grid()\n",
    "plt.xlabel('Return Period (yrs)')\n",
    "plt.ylabel('Value [m]');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Create design hydrographs \n",
    "\n",
    "To force a hydrodynamic model we need a timeseries of waterlevels, not just the maximum value from the extreme value analysis.\n",
    "In the next cell you will combine a simplified tidal signal with the extreme values to derive design hydrographs per return period. \n",
    "\n",
    "Fill in the variables `duration` and `mhw` in the code below. The duration give the duration of the surge hydrograph in hours, we will use 36 hours. The mean high water (mhw) is an estimate of the tide. You can find  this value at: https://tidesandcurrents.noaa.gov/datums.html?datum=MSL&units=1&epoch=0&id=8665530 (replace id=<station id> with your station id). To test, you can also use different values and see how it affects the hydrograph. \n",
    "    \n",
    "<u>**Q6: This approach uses several assumptions about the tide and surge. Which assumptions are taken and how valid are these?**<u>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q6 answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# User inputs: uncomment and fill in your values\n",
    "# duration = \"<your value>\"\n",
    "# mhw = \"<your value>\" \n",
    "\n",
    "# create a hydrograph based on mhw tide and triangular surge\n",
    "hydrograph = pd.DataFrame(\n",
    "    index = np.arange(-duration/2, duration/2+1),\n",
    "    data = {\n",
    "        'surge': np.zeros(duration+1),\n",
    "        # tide estimate based on cosine function with amplitude of mhw and period of 12 hours\n",
    "        'tide_mhw': np.cos(np.arange(-duration/2, duration/2+1)/12*np.pi)*mhw,\n",
    "    }\n",
    ")\n",
    "hydrograph.index.name = 'time'\n",
    "# create normalized triangular surge\n",
    "hydrograph.loc[:0, 'surge'] = np.linspace(0,1,int(duration/2)+1)\n",
    "hydrograph.loc[0:, 'surge'] = np.linspace(1,0,int(duration/2)+1)\n",
    "# combine triangular surge with mhw tide\n",
    "for rp in return_periods:\n",
    "    htot = df['absolute sealevel (m)'].loc[df['return periods (yrs)'] == rp].values[0]\n",
    "    surge = htot - mhw\n",
    "    hydrograph[rp] = hydrograph['surge'] * surge + hydrograph['tide_mhw']\n",
    "# remove normalized surge column\n",
    "hydrograph = hydrograph.drop(columns=['surge'])\n",
    "\n",
    "# to datetime (an actual date is required when setting the model forincg)\n",
    "t0 = pd.to_datetime('2021-01-01 00:00:00') # random start time of hydrograph\n",
    "hydrograph.index = pd.to_timedelta(hydrograph.index + duration/2, unit='h') + t0\n",
    "hydrograph = hydrograph.round(2) # round to cm \n",
    "\n",
    "# save the design hydrographs and water levels to csv\n",
    "hydrograph.to_csv(obs_folder.joinpath(f'design_hydrographs_{station_id}.csv'))\n",
    "\n",
    "# plot \n",
    "hydrograph.rename(columns={rp: f'{rp} yr' for rp in return_periods}, inplace=True)\n",
    "fig, ax = plt.subplots(1,1)\n",
    "hydrograph.plot(ax=ax)\n",
    "ax.set_title('design events')\n",
    "plt.ylabel('water level (m)')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus excercise\n",
    "Search for and download the COAST-RP dataset. This is a global dataset and contains return periods of total water levels. Open the dataset using xarray and search for the station that is nearest to Charleston, CS. Compare the water levels per return period. What differences do you observe? What could be the cause of this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#start with something like\n",
    "#import xarray as xr\n",
    "#ds = xr.open_dataset('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "integrated-modelling-course",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
